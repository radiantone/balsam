{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"User installation Balsam users should simply add Balsam to their environment with pip . git clone https://github.com/balsam-alcf/balsam.git cd balsam git checkout develop # Set up Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with flexible (unpinned) dependencies: pip install -e . Developer/server-side installation For Balsam development and server deployments, there are some additional requirements. Use make install-dev to install Balsam with the necessary dependencies. Direct server dependencies (e.g. FastAPI) are pinned to help with reproducible deployments. git clone https://github.com/balsam-alcf/balsam.git cd balsam git checkout develop # Set up Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with pinned deployment and dev dependencies: make install-dev # Set up pre-commit linting hooks: pre-commit install To view the docs in your browser: Navigate to top-level balsam directory (where mkdocs.yml is located) and run: mkdocs serve Follow the link to the documentation. Docs are markdown files in the balsam/docs subdirectory and can be edited on-the-fly. The changes will auto-refresh in the browser window.","title":"Home"},{"location":"#user-installation","text":"Balsam users should simply add Balsam to their environment with pip . git clone https://github.com/balsam-alcf/balsam.git cd balsam git checkout develop # Set up Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with flexible (unpinned) dependencies: pip install -e .","title":"User installation"},{"location":"#developerserver-side-installation","text":"For Balsam development and server deployments, there are some additional requirements. Use make install-dev to install Balsam with the necessary dependencies. Direct server dependencies (e.g. FastAPI) are pinned to help with reproducible deployments. git clone https://github.com/balsam-alcf/balsam.git cd balsam git checkout develop # Set up Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with pinned deployment and dev dependencies: make install-dev # Set up pre-commit linting hooks: pre-commit install","title":"Developer/server-side installation"},{"location":"#to-view-the-docs-in-your-browser","text":"Navigate to top-level balsam directory (where mkdocs.yml is located) and run: mkdocs serve Follow the link to the documentation. Docs are markdown files in the balsam/docs subdirectory and can be edited on-the-fly. The changes will auto-refresh in the browser window.","title":"To view the docs in your browser:"},{"location":"LICENSE/","text":"BSD 3-Clause License Copyright (c) 2019, UChicago Argonne LLC All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of UChicago Argonne LLC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"development/appdef/","text":"ApplicationDefinition Design Security All aspects of the App's execution are contained in a Site-local file and NEVER stored or accessed by the public API: Environment variables Preamble (module loads, etc..) Preprocess fxn Postprocess fxn Command template & parameters Stage in files Stage out files Error & timeout handling options The command is generated from a Jinja template that only takes pre-specified parameters. Each parameter is escaped with shlex.quote to prevent shell injection. All job processing must take place in job workdir. The job workdirs are strictly under the data/ subdirectory. Any stage-in destination is expressed as an relative path to the workdir. This prevents staging-in or out malicious code into apps/","title":"Defining Applications"},{"location":"development/appdef/#applicationdefinition-design","text":"","title":"ApplicationDefinition Design"},{"location":"development/appdef/#security","text":"All aspects of the App's execution are contained in a Site-local file and NEVER stored or accessed by the public API: Environment variables Preamble (module loads, etc..) Preprocess fxn Postprocess fxn Command template & parameters Stage in files Stage out files Error & timeout handling options The command is generated from a Jinja template that only takes pre-specified parameters. Each parameter is escaped with shlex.quote to prevent shell injection. All job processing must take place in job workdir. The job workdirs are strictly under the data/ subdirectory. Any stage-in destination is expressed as an relative path to the workdir. This prevents staging-in or out malicious code into apps/","title":"Security"},{"location":"development/client/","text":"The API Client In transitioning Balsam from a database-driven application to a multi-user, Web client-driven application, we have had to rethink how the Python API should look. Both internal Balsam components and user-written scripts need a way to manipulate and synchronize with the central state. In Balsam 0.x , users leverage direct access to the Django ORM and manipulate the database with simple APIs like: BalsamJob.objects.filter(state=\"FAILED\").delete() . Obviously, direct database access is not acceptable in a multi-user application. However, in cutting off access to the Django ORM, users would lose the familiar API (arguably one of Balsams' most important features) and have to drop down to writing and decoding JSON data for each request. The client architecure described below provides a solution to this problem with a Django ORM-inspired API. A familiar Python object model of the data, complete with models (e.g. Job ), managers ( Job.objects ), and Querysets ( Job.objects.filter(state=\"FAILED\").delete() ) is available. Instead of accessing a database, execution of these \"queries\" results in a REST API call. Internally, a RESTClient interface encapsulates the HTTP request and authentication logic and contains Resource components that map ordinary Python methods to API methods. The RESTClient interface All Python interactions with the Balsam REST API occur through the balsam.client.RESTClient interface. The base RESTClient is a composite class containing several Resource components that comprise the API's endpoints: users sites apps batch_jobs jobs events sessions In addition to these Resources , the RESTClient provides interfaces to perform the actual requests and authenticate to the API: class RESTClient: def interactive_login(self): \"\"\"Initiate interactive login flow\"\"\" def refresh_auth(self): \"\"\" Reload credentials if stored/not expired. Set appropriate Auth headers on HTTP session. \"\"\" def request(self, absolute_url, http_method, payload=None): \"\"\" Perform API request and return response data Supports timeout retry, auto re-authentication, accepting DUPLICATE status Raises helpful errors on 4**, 5**, TimeoutErrors, AuthErrors \"\"\" Subclasses of the RESTClient interface are responsible for these implementations. For instance, the DirectAPIClient makes a direct connection to a PostgreSQL Balsam database and passes request objects directly into the API views. This permits users who wish to control their own Balsam database to do so without running an actual web server. The RequestsClient uses the Python requests library to perform real HTTPS requests. It supports sessions with persistent connections and auto-retry of timed-out requests. Subclasses of RequestsClient can override refresh_auth() and interactive_login() to support different authentication schemes. The Resource components The base RESTClient implementation contains 7 Resource components: one for each path or logical resource in the API. Each Resource , in turn, contains a set of API action methods that build the URL and request payload, then invoke the RESTClient.request() method to perform the communication. The generic actions look as follows: class Resource: def list(self, **query_params): \"\"\"Returns a collection of items matching query\"\"\" url = self.client.build_url(self.collection_path, **query_params) response = self.client.request(url, \"GET\") return self.client.extract_data(response) def detail(self, uri, **query_params): \"\"\"Retrieve one object from collection\"\"\" def create(self, **payload): \"\"\"Create new item; returns ID\"\"\" def update(self, uri, payload, partial=False, **query_params): \"\"\"Update an exisitng item\"\"\" def bulk_create(self, list_payload): \"\"\"Bulk create list of items\"\"\" def bulk_update_query(self, patch, **query_params): \"\"\"Apply same patch to every item matched by query\"\"\" def bulk_update_patch(self, patch_list): \"\"\"Applies a list of patches item-wise\"\"\" def destroy(self, uri, **query_params): \"\"\"Delete the item given by the uri\"\"\" def bulk_destroy(self, **query_params): \"\"\"Delete the items matched by the query\"\"\" The Resources contain a circular reference back to their parent RESTClient to use the helper methods build_url and extract_data as well as invoke request . The uniform, flat structure of the REST API allows the simple base Resource to be used for most API actions. For resources with specialized API calls, Resource is be subclassed to provide the appropriate Python methods. For example, client.jobs is a derived class named JobResource providing the additional method history() , so that users may fetch all the events for a particular job via client.jobs.history(uri) . A call to client.jobs.history(3) results in HTTP GET request to the path jobs/3/events .","title":"The Python REST Client"},{"location":"development/client/#the-api-client","text":"In transitioning Balsam from a database-driven application to a multi-user, Web client-driven application, we have had to rethink how the Python API should look. Both internal Balsam components and user-written scripts need a way to manipulate and synchronize with the central state. In Balsam 0.x , users leverage direct access to the Django ORM and manipulate the database with simple APIs like: BalsamJob.objects.filter(state=\"FAILED\").delete() . Obviously, direct database access is not acceptable in a multi-user application. However, in cutting off access to the Django ORM, users would lose the familiar API (arguably one of Balsams' most important features) and have to drop down to writing and decoding JSON data for each request. The client architecure described below provides a solution to this problem with a Django ORM-inspired API. A familiar Python object model of the data, complete with models (e.g. Job ), managers ( Job.objects ), and Querysets ( Job.objects.filter(state=\"FAILED\").delete() ) is available. Instead of accessing a database, execution of these \"queries\" results in a REST API call. Internally, a RESTClient interface encapsulates the HTTP request and authentication logic and contains Resource components that map ordinary Python methods to API methods.","title":"The API Client"},{"location":"development/client/#the-restclient-interface","text":"All Python interactions with the Balsam REST API occur through the balsam.client.RESTClient interface. The base RESTClient is a composite class containing several Resource components that comprise the API's endpoints: users sites apps batch_jobs jobs events sessions In addition to these Resources , the RESTClient provides interfaces to perform the actual requests and authenticate to the API: class RESTClient: def interactive_login(self): \"\"\"Initiate interactive login flow\"\"\" def refresh_auth(self): \"\"\" Reload credentials if stored/not expired. Set appropriate Auth headers on HTTP session. \"\"\" def request(self, absolute_url, http_method, payload=None): \"\"\" Perform API request and return response data Supports timeout retry, auto re-authentication, accepting DUPLICATE status Raises helpful errors on 4**, 5**, TimeoutErrors, AuthErrors \"\"\" Subclasses of the RESTClient interface are responsible for these implementations. For instance, the DirectAPIClient makes a direct connection to a PostgreSQL Balsam database and passes request objects directly into the API views. This permits users who wish to control their own Balsam database to do so without running an actual web server. The RequestsClient uses the Python requests library to perform real HTTPS requests. It supports sessions with persistent connections and auto-retry of timed-out requests. Subclasses of RequestsClient can override refresh_auth() and interactive_login() to support different authentication schemes.","title":"The RESTClient interface"},{"location":"development/client/#the-resource-components","text":"The base RESTClient implementation contains 7 Resource components: one for each path or logical resource in the API. Each Resource , in turn, contains a set of API action methods that build the URL and request payload, then invoke the RESTClient.request() method to perform the communication. The generic actions look as follows: class Resource: def list(self, **query_params): \"\"\"Returns a collection of items matching query\"\"\" url = self.client.build_url(self.collection_path, **query_params) response = self.client.request(url, \"GET\") return self.client.extract_data(response) def detail(self, uri, **query_params): \"\"\"Retrieve one object from collection\"\"\" def create(self, **payload): \"\"\"Create new item; returns ID\"\"\" def update(self, uri, payload, partial=False, **query_params): \"\"\"Update an exisitng item\"\"\" def bulk_create(self, list_payload): \"\"\"Bulk create list of items\"\"\" def bulk_update_query(self, patch, **query_params): \"\"\"Apply same patch to every item matched by query\"\"\" def bulk_update_patch(self, patch_list): \"\"\"Applies a list of patches item-wise\"\"\" def destroy(self, uri, **query_params): \"\"\"Delete the item given by the uri\"\"\" def bulk_destroy(self, **query_params): \"\"\"Delete the items matched by the query\"\"\" The Resources contain a circular reference back to their parent RESTClient to use the helper methods build_url and extract_data as well as invoke request . The uniform, flat structure of the REST API allows the simple base Resource to be used for most API actions. For resources with specialized API calls, Resource is be subclassed to provide the appropriate Python methods. For example, client.jobs is a derived class named JobResource providing the additional method history() , so that users may fetch all the events for a particular job via client.jobs.history(uri) . A call to client.jobs.history(3) results in HTTP GET request to the path jobs/3/events .","title":"The Resource components"},{"location":"development/contributing/","text":"Developer Guidelines Porting to new HPC systems To port Balsam to a new system, a developer should only need to implement the following platform interfaces: platform/app_run : Add AppRun subclass and list it in the init .py platform/compute_node : Same for ComputeNode platform/scheduler : Same for SchedulerInterface Then create a new default configuration folder for the Site under balsam/config/defaults . This isn't strictly necessary (users can write their own config files) but it makes it very convenient for others to quickly spin up a Site with the interfaces you wrote. You will need the following inside the default Site configuration directory: apps/__init__.py (and other default apps therein) settings.yml (Referencing the platform interfaces added above) job-template.sh Developer Installation # Use a Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with deployment/development dependencies: make install-dev # Set up pre-commit linting hooks: pre-commit install On commit, code will be auto-formatted with isort and black and linted with flake8 . Linting errors will cause the commit to fail and point to errors. Contributors may also run the following to re-format, lint, type-check, and test the code: $ make format $ make all Please run these steps before making pull requests. Creating diagrams in markdown Refer to mermaid.js for examples on graphs, flowcharts, sequence diagrams, class diagrams, state diagrams, etc... graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2]","title":"Contribution Guide"},{"location":"development/contributing/#developer-guidelines","text":"","title":"Developer Guidelines"},{"location":"development/contributing/#porting-to-new-hpc-systems","text":"To port Balsam to a new system, a developer should only need to implement the following platform interfaces: platform/app_run : Add AppRun subclass and list it in the init .py platform/compute_node : Same for ComputeNode platform/scheduler : Same for SchedulerInterface Then create a new default configuration folder for the Site under balsam/config/defaults . This isn't strictly necessary (users can write their own config files) but it makes it very convenient for others to quickly spin up a Site with the interfaces you wrote. You will need the following inside the default Site configuration directory: apps/__init__.py (and other default apps therein) settings.yml (Referencing the platform interfaces added above) job-template.sh","title":"Porting to new HPC systems"},{"location":"development/contributing/#developer-installation","text":"# Use a Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with deployment/development dependencies: make install-dev # Set up pre-commit linting hooks: pre-commit install On commit, code will be auto-formatted with isort and black and linted with flake8 . Linting errors will cause the commit to fail and point to errors. Contributors may also run the following to re-format, lint, type-check, and test the code: $ make format $ make all Please run these steps before making pull requests.","title":"Developer Installation"},{"location":"development/contributing/#creating-diagrams-in-markdown","text":"Refer to mermaid.js for examples on graphs, flowcharts, sequence diagrams, class diagrams, state diagrams, etc... graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2]","title":"Creating diagrams in markdown"},{"location":"development/data-model/","text":"Understanding Balsam Balsam is made up of: A centrally-managed, multi-tenant web application for securely curating HPC applications, authoring workflows, and managing high-throughput job campaigns across one or many computing facilities. Distributed, user-run Balsam Sites that sync with the central API to orchestrate and carry out the workflows defined by users on a given HPC platform. In order to understand how Balsam is organized, one should first consider the server side entities. This graph shows the database schema of the Balsam application. Each node is a table in the database, represented by one of the model classes in the ORM. Each arrow represents a ForeignKey (or many-to-one ) relationship between two tables. The Database Schema A User represents a Balsam user account. All items in the database are linked to a single owner (tenant) , which is reflected in the connectivity of the graph. For example, to get all the jobs belonging to current_user , join the tables via Job.objects.filter(app__site__user=current_user) A Site is uniquely identified as a directory on some machine: (machine hostname, directory path) . One user can own several Balsam sites located across one or several machines. Each site is an independent endpoint where applications are registered, data is transferred in and out, and Job working directories are located. Each Balsam site runs a daemon on behalf of the user that communicates with the central API. If a user has multiple active Balsam Sites, then a separate daemon runs at each of them. The authenticated daemons communicate with the central Balsam API to fetch jobs, orchestrate the workflow locally, and update the database state. An App represents a runnable application at a particular Balsam Site. Every Balsam Site contains an apps/ directory with Python modules containing ApplicationDefinition classes. The set of ApplicationDefinitions determines the applications which may run at the Site. An App instance in the data model is merely a reference to an ApplicationDefinition class, uniquely identified by the Site ID and class path. A Job represents a single run of an App at a particular Site . The Job contains both application-specific data (like command line arguments) and resource requirements (like number of MPI ranks per node) for the run. It is important to note that Job-->App-->Site are non-nullable relations, so a Job is always bound to run at a particular Site from the moment its created . Therefore, the corresponding Balsam service daemon may begin staging-in data as soon as a Job becomes visible, as appropriate. A BatchJob represents a job launch script and resource request submitted by the Site to the local workload manager (e.g. Slurm). Notice that the relation of BatchJob to Site is many-to-one, and that Job to BatchJob is many-to-one. That is, many Jobs run in a single BatchJob , and many BatchJobs are submitted at a Site over time. The Session is an internal model representing an active Balsam launcher session. Jobs have a nullable relationship to Session ; when it is not null, the job is said to be locked by a launcher, and no other launcher should try running it. The Balsam session API is used by launchers acquiring jobs concurrently to avoid race conditions. Sessions contain a heartbeat timestamp that must be periodically ticked to maintain the session. A TransferItem is created for each stage-in or stage-out task associated with a Job . This permits the transfer module of the Balsam service to group transfers according to the remote source or destination, and therefore batch small transfers efficiently. When all the stage-in TransferItems linked to a Job are finished, it is considered \"staged-in\" and moves ahead to preprocessing. A LogEvent contains a timestamp , from_state , to_state , and message for each state transition linked to a Job . The benefit of breaking a Job's state history out into a separate Table is that it becomes easy to query for aggregate throughput, etc... without having to first parse and accumulate timestamps nested inside a Job field. The REST API Refer to the interactive document located under the /docs URL of your Balsam server for detailed information about each endpoint. For instance, launch a local server with docker-compose up and visit localhost:8000/docs . User & a note on Auth Generally, Balsam will need two types of Auth to function: Login auth: This will likely be an pair of views providing an OAuth flow, where Balsam redirects the user to an external auth system, and upon successful authentication, user information is redirected back to a Balsam callback view. For testing purposes, basic password-based login could be used instead. Token auth: After the initial login, Balsam clients need a way to authenticate subsequent requests to the API. This can be performed with Token authentication and a secure setup like Django REST Knox . Upon successful login authentication (step 1), a Token is generated and stored (encrypted) for the User. This token is returned to the client in the login response. The client then stores this token, which has some expiration date, and includes it as a HTTP header on every subsequent request to the API (e.g. Authorization: Token 4789ac8372... ). This is both how Javascript web clients and automated Balsam Site services can communicate with the API. Summary of Endpoints HTTP Method URL Description Example usage GET /sites/ Retrieve the current user's list of sites A user checks their Balsam site statuses on dashboard POST /sites/ Create a new Site balsam init creates a Site and stores new id locally PUT /sites/{id} Update Site information Service daemon syncs backfill_windows periodically DELETE /sites/{id} Delete Site User deletes their Site with balsam rm site ----------- --------------- -------------------- ------------------- GET /apps/ Retrieve the current user's list of Apps balsam ls apps shows Apps across sites POST /apps/ Create a new App balsam app sync creates new Apps from local ApplicationDefinitions PUT /apps/{id} Update App information balsam app sync updates existing Apps with changes from local ApplicationDefinitions DELETE /apps/{id} Delete App User deletes an App ; all related Jobs are deleted ----------- --------------- -------------------- ------------------- GET /jobs/ Get paginated Job lists, filtered by site, state, tags, BatchJob, or App balsam ls POST /jobs/ Bulk-create Jobs Create 1k jobs with single API call PUT /jobs/{id} Update Job information Tweak a single job in web UI DELETE /jobs/{id} Delete Job Delete a single job in web UI PUT /jobs/ Bulk-update Jobs: apply same update to all jobs matching query Restart all jobs at Site X with tag workflow=\"foo\" PATCH /jobs/ Bulk-update Jobs: apply list of patches job-wise Balsam StatusUpdater component sends a list of status updates to API ----------- --------------- -------------------- ------------------- GET /batch-jobs/ Get BatchJobs Web client lists recent BatchJobs POST /batch-jobs/ Create BatchJob Web client or AutoScaler submits a new BatchJob PUT /batch-jobs/{id} Alter BatchJob by ID Web client alters job runtime while queued DELETE /batch-jobs/{id} Delete BatchJob by ID User deletes job before it was ever submitted PATCH /batch-jobs/ Bulk Update batch jobs by patch list Service syncs BatchJob states ----------- --------------- -------------------- ------------------- GET /sessions Get Sessions List BatchJob Web view shows \"Last Heartbeat\" for each running POST /sessions Create new Session Launcher JobSource initialized POST /sessions/{id}/acquire Acquire Jobs for launcher JobSource acquires new jobs to run PUT /sessions/{id} Tick Session heartbeat JobSource ticks Session periodically DELETE /sessions/{id} Destroy Session and release Jobs Final JobSource release() call ----------- --------------- -------------------- ------------------- GET /transfers/ List TransferItems Transfer module gets list of pending Transfers PUT /transfers/{id} Update TransferItem State Transfer module updates status PATCH /transfers/ Bulk update TransferItems via patch list Transfer module bulk-updates statuses of finished transfers ----------- --------------- -------------------- ------------------- GET /events Fetch EventLogs Web client filters by Job tags and last 24 hours to get a quick view at throughput/utilization for a particular job type Site Field Name Description id Unique Site ID hostname The server address or hostname like thetalogin3.theta.alcf.anl.gov path Absolute POSIX path to the Site directory last_refresh Automatically updated timestamp: last update to Site information creation_date Timestamp when Site was created owner ForeignKey to User model globus_endpoint_id Optional UUID : setting an associated endpoint for data transfer num_nodes Number of compute nodes available at the Site backfill_windows JSONField: array of [queue, num_nodes, wall_time_min] tuples indicating backfill slots queued_jobs JSONField: array of [queue, num_nodes, wall_time_min, state] indicating currently queued and running jobs optional_batch_job_params JSONField used in BatchJob forms/validation {name: default_value} . Taken from site config. allowed_projects JSONField used in BatchJob forms/validation: [ name: str ] allowed_queues JSONField used in BatchJob forms/validation: {name: {max_nodes, max_walltime, max_queued}} transfer_locations JSONField used in Job stage-in/stage-out validation: {alias: {protocol, netloc}} App Field Name Description id Unique App ID site Foreign Key to Site instance containing this App name Short name identifying the app. description Text description (useful in generating Web forms) class_path Name of ApplicationDefinition class in the format: {module_name}.{class_name} parameters Command line template parameters. A dict of dicts with the structure: {name: {required: bool, default: str, help: str}} transfers A dict of stage-in/stage-out slots with the structure: {name: {required: bool, direction: [\"in\"|\"out\"], target_path: str, help: str}} The App model is used to merely index the ApplicationDefinition classes that a user has registered at their Balsam Sites. The parameters field represents \"slots\" for each adjustable command line parameter. For example, an ApplicationDefinition command template of \"echo hello, {{first_name}}!\" would result in an App having the parameters list: [ {name: \"first_name\", required: true, default: \"\", help: \"\"} ] . None of the Balsam site components use App.parameters internally; the purpose of mirroring this field in the database is simply to facilitate Job validation and create App-tailored web forms. Similarly, transfers mirrors data on the ApplicationDefinition for Job input and validation purposes only. For security reasons, the validation of Job input parameters takes place in the site-local ApplicationDefinition module. Even if a malicious user altered the parameters field in the API, they would not be able to successfully run a Job with injected parameters. Job Field Name Description id Unique Job ID workdir Working directory, relative to the Site data/ directory tags JSON {str: str} mappings for tagging and selecting jobs session ForeignKey to Session instance app ForeignKey to App instance parameters JSON {paramName: paramValue} for the App command template parameters batch_job ForeignKey to current or most recent BatchJob instance in which this Job ran state Current state of the Job last_update Timestamp of last modification to Job data Arbitrary JSON data storage return_code Most recent return code of job parents Non-symmetric ManyToMany Parent --> Child relations between Jobs num_nodes Number of compute nodes required (> 1 implies MPI usage) ranks_per_node Number of ranks per node (> 1 implies MPI usage) threads_per_rank Number of logical threads per MPI rank threads_per_core Number of logical threads per hardware core launch_params Optional pass-through parameters to MPI launcher (e.g -cc depth) gpus_per_rank Number of GPUs per MPI rank node_packing_count Maximum number of instances that can run on a single node wall_time_min Lower bound estimate for runtime of the Job (leaving at default 0 is allowed) Let workdir uniqueness be the user's problem. If they put 2 jobs with same workdir, assume it's intentional. We can ensure that \"stdout\" of each job goes into a file named by Job ID, so multiple runs do not collide. stateDiagram-v2 created: Created awaiting_parents: Awaiting Parents ready: Ready staged_in: Staged In preprocessed: Preprocessed restart_ready: Restart Ready running: Running run_done: Run Done postprocessed: Postprocessed staged_out: Staged Out finished: Job Finished run_error: Run Error run_timeout: Run Timeout failed: Failed created --> ready: No parents created --> awaiting_parents: Pending dependencies awaiting_parents --> ready: Dependencies finished ready --> staged_in: Transfer external data in staged_in --> preprocessed: Run preprocess script preprocessed --> running: Launch job running --> run_done: Return code 0 running --> run_error: Nonzero return running --> run_timeout: Early termination run_timeout --> restart_ready: Auto retry run_error --> restart_ready: Run error handler run_error --> failed: No error handler restart_ready --> running: Launch job run_done --> postprocessed: Run postprocess script postprocessed --> staged_out: Transfer data out staged_out --> finished: Job Finished A user can only access Jobs they own. The related App, BatchJob, and parents are included by ID in the serialized representation. The session is excluded since it is only used internally. Reverse relationships (one-to-many) with transfers and events are also not included in the Job representation, as they can be accessed through separate API endpoints. The related entities are represented in JSON as follows: Field Serialized Deserialized id Primary Key Fetch Job from user-filtered queryset app_id Primary Key Fetch App from user-filtered queryset batch_job_id Primary Key Fetch BatchJob from user-filtered queryset parent_ids Primary Key list Fetch parent jobs from user-filtered queryset transfers N/A Create only: Dict of {transfer_item_name: {location_alias: str, path: str}} events N/A N/A session N/A N/A transfers are nested in the Job for POST only: Job creation is an atomic transaction grouping addition of the Job with its related TransferItems . The API fetches the related App.transfers and Site.transfer_locations to validate each transfer item: transfer_item_name must match one of the keys in App.transfers , which determines the direction and local path The location_alias must match one of the keys in Site.transfer_locations , which determines the protocol and remote_netloc Finally, the remote path is determined by the path key in each Job transfer item BatchJob Field Name Description id Unique ID. Not to be confused with Scheduler ID, which is not necessarily unique across Sites! site ForeignKey to Site where submitted scheduler_id ID assigned by Site's batch scheduler (null if unassigned) project Project/allocation to be charged for the job submission queue Which scheduler queue the batchjob is submitted to num_nodes Number of nodes requested for batchjob wall_time_min Wall time, in minutes, requested job_mode Balsam launcher job mode optional_params Extra pass-through parameters to Job Template filter_tags Restrict launcher to run jobs with matching tags. JSONField dict: {tag_key: tag_val} state Current status of BatchJob status_info JSON: Error or custom data received from scheduler start_time DateTime when BatchJob started running end_time DateTime when BatchJob ended Every workload manager is different and there are numerous job states intentionally not considered in the BatchJob model, including starting , exiting , user_hold , dep_hold , etc. It is the responsibility of the site's Scheduler interface to translate real scheduler states to one of the few coarse-grained Balsam BatchJob states: queued , running , or finished . stateDiagram-v2 pending_submission --> queued pending_submission --> submit_failed queued --> running running --> finished pending_submission --> pending_deletion queued --> pending_deletion running --> pending_deletion pending_deletion --> finished Session Field Name Description id Unique ID heartbeat DateTime of last session tick API call batch_job Non-nullable ForeignKey to BatchJob this Session is running under Session creation only requires providing batch_job_id . Session tick has empty payload Session acquire endpoint uses a special JobAcquireSerializer representation: Field Description states list of states to acquire max_num_acquire limit number of jobs to acquire filter_tags filter Jobs for which job.tags contains all {tag_name: tag_value} pairs node_resources Nested NodeResource representation placing resource constraints on what Jobs may be acquired order_by order returned jobs according to a set of Job fields (may include ascending or descending num_nodes , node_packing_count , wall_time_min ) The nested NodeResource representation is provided as a dict with the structure: { \"max_jobs_per_node\" : 1 , # Determined by Site settings for each Launcher job mode \"max_wall_time_min\" : 60 , \"running_job_counts\" : [ 0 , 1 , 0 ], \"node_occupancies\" : [ 0.0 , 1.0 , 0.0 ], \"idle_cores\" : [ 64 , 63 , 64 ], \"idle_gpus\" : [ 1 , 0 , 1 ], } TransferItem Field Name Description id Unique TransferItem ID job ForeignKey to Job protocol globus or rsync direction in or out . If in , the transfer is from remote_netloc:source_path to Job.workdir/destination_path . If out , the transfer is from Job.workdir/src_path to remote_netloc:dest_path . remote_netloc The Globus endpoint UUID or user@hostname of the remote data location source_path If stage- in : the remote path. If stage- out : the local path destination_path If stage- in : the local path. If stage- out : the remote path. state pending -> active -> done or error task_id Unique identifier of the Transfer task (e.g. Globus Task UUID) transfer_info JSONField for Error messages, average bandwidth, transfer time, etc... There is no create ( POST ) method on the /transfers endpoint, because TransferItem creation is directly linked with Job creation. The related Transfers are nested in the Job representation when POSTING new jobs. The following fields are fixed at creation time: id job protocol direction remote_netloc source_path dest_path For list (GET), the representation includes all fields. job_id represents the Job by primary key. For update (PUT and PATCH), only state , task_id , and transfer_info may be modified. The update of a state to done triggers a check of the related Job 's transfers to determine whether the job can be advanced to STAGED_IN . LogEvent Field Name Description id Unique ID job ForeignKey to Job undergoing event timestamp DateTime of event from_state Job state before transition to_state Job state after transition data JSONField containing {message: str} and other optional data For transitions to or from RUNNING , the data includes nodes as a fractional number of occupied nodes. This enables clients to generate throughput and utilization views without having to fetch entire related Jobs. This is a read only-API with all fields included. The related Job is represented by primary key job_id field.","title":"Understanding Balsam"},{"location":"development/data-model/#understanding-balsam","text":"Balsam is made up of: A centrally-managed, multi-tenant web application for securely curating HPC applications, authoring workflows, and managing high-throughput job campaigns across one or many computing facilities. Distributed, user-run Balsam Sites that sync with the central API to orchestrate and carry out the workflows defined by users on a given HPC platform. In order to understand how Balsam is organized, one should first consider the server side entities. This graph shows the database schema of the Balsam application. Each node is a table in the database, represented by one of the model classes in the ORM. Each arrow represents a ForeignKey (or many-to-one ) relationship between two tables.","title":"Understanding Balsam"},{"location":"development/data-model/#the-database-schema","text":"A User represents a Balsam user account. All items in the database are linked to a single owner (tenant) , which is reflected in the connectivity of the graph. For example, to get all the jobs belonging to current_user , join the tables via Job.objects.filter(app__site__user=current_user) A Site is uniquely identified as a directory on some machine: (machine hostname, directory path) . One user can own several Balsam sites located across one or several machines. Each site is an independent endpoint where applications are registered, data is transferred in and out, and Job working directories are located. Each Balsam site runs a daemon on behalf of the user that communicates with the central API. If a user has multiple active Balsam Sites, then a separate daemon runs at each of them. The authenticated daemons communicate with the central Balsam API to fetch jobs, orchestrate the workflow locally, and update the database state. An App represents a runnable application at a particular Balsam Site. Every Balsam Site contains an apps/ directory with Python modules containing ApplicationDefinition classes. The set of ApplicationDefinitions determines the applications which may run at the Site. An App instance in the data model is merely a reference to an ApplicationDefinition class, uniquely identified by the Site ID and class path. A Job represents a single run of an App at a particular Site . The Job contains both application-specific data (like command line arguments) and resource requirements (like number of MPI ranks per node) for the run. It is important to note that Job-->App-->Site are non-nullable relations, so a Job is always bound to run at a particular Site from the moment its created . Therefore, the corresponding Balsam service daemon may begin staging-in data as soon as a Job becomes visible, as appropriate. A BatchJob represents a job launch script and resource request submitted by the Site to the local workload manager (e.g. Slurm). Notice that the relation of BatchJob to Site is many-to-one, and that Job to BatchJob is many-to-one. That is, many Jobs run in a single BatchJob , and many BatchJobs are submitted at a Site over time. The Session is an internal model representing an active Balsam launcher session. Jobs have a nullable relationship to Session ; when it is not null, the job is said to be locked by a launcher, and no other launcher should try running it. The Balsam session API is used by launchers acquiring jobs concurrently to avoid race conditions. Sessions contain a heartbeat timestamp that must be periodically ticked to maintain the session. A TransferItem is created for each stage-in or stage-out task associated with a Job . This permits the transfer module of the Balsam service to group transfers according to the remote source or destination, and therefore batch small transfers efficiently. When all the stage-in TransferItems linked to a Job are finished, it is considered \"staged-in\" and moves ahead to preprocessing. A LogEvent contains a timestamp , from_state , to_state , and message for each state transition linked to a Job . The benefit of breaking a Job's state history out into a separate Table is that it becomes easy to query for aggregate throughput, etc... without having to first parse and accumulate timestamps nested inside a Job field.","title":"The Database Schema"},{"location":"development/data-model/#the-rest-api","text":"Refer to the interactive document located under the /docs URL of your Balsam server for detailed information about each endpoint. For instance, launch a local server with docker-compose up and visit localhost:8000/docs .","title":"The REST API"},{"location":"development/data-model/#user-a-note-on-auth","text":"Generally, Balsam will need two types of Auth to function: Login auth: This will likely be an pair of views providing an OAuth flow, where Balsam redirects the user to an external auth system, and upon successful authentication, user information is redirected back to a Balsam callback view. For testing purposes, basic password-based login could be used instead. Token auth: After the initial login, Balsam clients need a way to authenticate subsequent requests to the API. This can be performed with Token authentication and a secure setup like Django REST Knox . Upon successful login authentication (step 1), a Token is generated and stored (encrypted) for the User. This token is returned to the client in the login response. The client then stores this token, which has some expiration date, and includes it as a HTTP header on every subsequent request to the API (e.g. Authorization: Token 4789ac8372... ). This is both how Javascript web clients and automated Balsam Site services can communicate with the API.","title":"User &amp; a note on Auth"},{"location":"development/data-model/#summary-of-endpoints","text":"HTTP Method URL Description Example usage GET /sites/ Retrieve the current user's list of sites A user checks their Balsam site statuses on dashboard POST /sites/ Create a new Site balsam init creates a Site and stores new id locally PUT /sites/{id} Update Site information Service daemon syncs backfill_windows periodically DELETE /sites/{id} Delete Site User deletes their Site with balsam rm site ----------- --------------- -------------------- ------------------- GET /apps/ Retrieve the current user's list of Apps balsam ls apps shows Apps across sites POST /apps/ Create a new App balsam app sync creates new Apps from local ApplicationDefinitions PUT /apps/{id} Update App information balsam app sync updates existing Apps with changes from local ApplicationDefinitions DELETE /apps/{id} Delete App User deletes an App ; all related Jobs are deleted ----------- --------------- -------------------- ------------------- GET /jobs/ Get paginated Job lists, filtered by site, state, tags, BatchJob, or App balsam ls POST /jobs/ Bulk-create Jobs Create 1k jobs with single API call PUT /jobs/{id} Update Job information Tweak a single job in web UI DELETE /jobs/{id} Delete Job Delete a single job in web UI PUT /jobs/ Bulk-update Jobs: apply same update to all jobs matching query Restart all jobs at Site X with tag workflow=\"foo\" PATCH /jobs/ Bulk-update Jobs: apply list of patches job-wise Balsam StatusUpdater component sends a list of status updates to API ----------- --------------- -------------------- ------------------- GET /batch-jobs/ Get BatchJobs Web client lists recent BatchJobs POST /batch-jobs/ Create BatchJob Web client or AutoScaler submits a new BatchJob PUT /batch-jobs/{id} Alter BatchJob by ID Web client alters job runtime while queued DELETE /batch-jobs/{id} Delete BatchJob by ID User deletes job before it was ever submitted PATCH /batch-jobs/ Bulk Update batch jobs by patch list Service syncs BatchJob states ----------- --------------- -------------------- ------------------- GET /sessions Get Sessions List BatchJob Web view shows \"Last Heartbeat\" for each running POST /sessions Create new Session Launcher JobSource initialized POST /sessions/{id}/acquire Acquire Jobs for launcher JobSource acquires new jobs to run PUT /sessions/{id} Tick Session heartbeat JobSource ticks Session periodically DELETE /sessions/{id} Destroy Session and release Jobs Final JobSource release() call ----------- --------------- -------------------- ------------------- GET /transfers/ List TransferItems Transfer module gets list of pending Transfers PUT /transfers/{id} Update TransferItem State Transfer module updates status PATCH /transfers/ Bulk update TransferItems via patch list Transfer module bulk-updates statuses of finished transfers ----------- --------------- -------------------- ------------------- GET /events Fetch EventLogs Web client filters by Job tags and last 24 hours to get a quick view at throughput/utilization for a particular job type","title":"Summary of Endpoints"},{"location":"development/data-model/#site","text":"Field Name Description id Unique Site ID hostname The server address or hostname like thetalogin3.theta.alcf.anl.gov path Absolute POSIX path to the Site directory last_refresh Automatically updated timestamp: last update to Site information creation_date Timestamp when Site was created owner ForeignKey to User model globus_endpoint_id Optional UUID : setting an associated endpoint for data transfer num_nodes Number of compute nodes available at the Site backfill_windows JSONField: array of [queue, num_nodes, wall_time_min] tuples indicating backfill slots queued_jobs JSONField: array of [queue, num_nodes, wall_time_min, state] indicating currently queued and running jobs optional_batch_job_params JSONField used in BatchJob forms/validation {name: default_value} . Taken from site config. allowed_projects JSONField used in BatchJob forms/validation: [ name: str ] allowed_queues JSONField used in BatchJob forms/validation: {name: {max_nodes, max_walltime, max_queued}} transfer_locations JSONField used in Job stage-in/stage-out validation: {alias: {protocol, netloc}}","title":"Site"},{"location":"development/data-model/#app","text":"Field Name Description id Unique App ID site Foreign Key to Site instance containing this App name Short name identifying the app. description Text description (useful in generating Web forms) class_path Name of ApplicationDefinition class in the format: {module_name}.{class_name} parameters Command line template parameters. A dict of dicts with the structure: {name: {required: bool, default: str, help: str}} transfers A dict of stage-in/stage-out slots with the structure: {name: {required: bool, direction: [\"in\"|\"out\"], target_path: str, help: str}} The App model is used to merely index the ApplicationDefinition classes that a user has registered at their Balsam Sites. The parameters field represents \"slots\" for each adjustable command line parameter. For example, an ApplicationDefinition command template of \"echo hello, {{first_name}}!\" would result in an App having the parameters list: [ {name: \"first_name\", required: true, default: \"\", help: \"\"} ] . None of the Balsam site components use App.parameters internally; the purpose of mirroring this field in the database is simply to facilitate Job validation and create App-tailored web forms. Similarly, transfers mirrors data on the ApplicationDefinition for Job input and validation purposes only. For security reasons, the validation of Job input parameters takes place in the site-local ApplicationDefinition module. Even if a malicious user altered the parameters field in the API, they would not be able to successfully run a Job with injected parameters.","title":"App"},{"location":"development/data-model/#job","text":"Field Name Description id Unique Job ID workdir Working directory, relative to the Site data/ directory tags JSON {str: str} mappings for tagging and selecting jobs session ForeignKey to Session instance app ForeignKey to App instance parameters JSON {paramName: paramValue} for the App command template parameters batch_job ForeignKey to current or most recent BatchJob instance in which this Job ran state Current state of the Job last_update Timestamp of last modification to Job data Arbitrary JSON data storage return_code Most recent return code of job parents Non-symmetric ManyToMany Parent --> Child relations between Jobs num_nodes Number of compute nodes required (> 1 implies MPI usage) ranks_per_node Number of ranks per node (> 1 implies MPI usage) threads_per_rank Number of logical threads per MPI rank threads_per_core Number of logical threads per hardware core launch_params Optional pass-through parameters to MPI launcher (e.g -cc depth) gpus_per_rank Number of GPUs per MPI rank node_packing_count Maximum number of instances that can run on a single node wall_time_min Lower bound estimate for runtime of the Job (leaving at default 0 is allowed) Let workdir uniqueness be the user's problem. If they put 2 jobs with same workdir, assume it's intentional. We can ensure that \"stdout\" of each job goes into a file named by Job ID, so multiple runs do not collide. stateDiagram-v2 created: Created awaiting_parents: Awaiting Parents ready: Ready staged_in: Staged In preprocessed: Preprocessed restart_ready: Restart Ready running: Running run_done: Run Done postprocessed: Postprocessed staged_out: Staged Out finished: Job Finished run_error: Run Error run_timeout: Run Timeout failed: Failed created --> ready: No parents created --> awaiting_parents: Pending dependencies awaiting_parents --> ready: Dependencies finished ready --> staged_in: Transfer external data in staged_in --> preprocessed: Run preprocess script preprocessed --> running: Launch job running --> run_done: Return code 0 running --> run_error: Nonzero return running --> run_timeout: Early termination run_timeout --> restart_ready: Auto retry run_error --> restart_ready: Run error handler run_error --> failed: No error handler restart_ready --> running: Launch job run_done --> postprocessed: Run postprocess script postprocessed --> staged_out: Transfer data out staged_out --> finished: Job Finished A user can only access Jobs they own. The related App, BatchJob, and parents are included by ID in the serialized representation. The session is excluded since it is only used internally. Reverse relationships (one-to-many) with transfers and events are also not included in the Job representation, as they can be accessed through separate API endpoints. The related entities are represented in JSON as follows: Field Serialized Deserialized id Primary Key Fetch Job from user-filtered queryset app_id Primary Key Fetch App from user-filtered queryset batch_job_id Primary Key Fetch BatchJob from user-filtered queryset parent_ids Primary Key list Fetch parent jobs from user-filtered queryset transfers N/A Create only: Dict of {transfer_item_name: {location_alias: str, path: str}} events N/A N/A session N/A N/A transfers are nested in the Job for POST only: Job creation is an atomic transaction grouping addition of the Job with its related TransferItems . The API fetches the related App.transfers and Site.transfer_locations to validate each transfer item: transfer_item_name must match one of the keys in App.transfers , which determines the direction and local path The location_alias must match one of the keys in Site.transfer_locations , which determines the protocol and remote_netloc Finally, the remote path is determined by the path key in each Job transfer item","title":"Job"},{"location":"development/data-model/#batchjob","text":"Field Name Description id Unique ID. Not to be confused with Scheduler ID, which is not necessarily unique across Sites! site ForeignKey to Site where submitted scheduler_id ID assigned by Site's batch scheduler (null if unassigned) project Project/allocation to be charged for the job submission queue Which scheduler queue the batchjob is submitted to num_nodes Number of nodes requested for batchjob wall_time_min Wall time, in minutes, requested job_mode Balsam launcher job mode optional_params Extra pass-through parameters to Job Template filter_tags Restrict launcher to run jobs with matching tags. JSONField dict: {tag_key: tag_val} state Current status of BatchJob status_info JSON: Error or custom data received from scheduler start_time DateTime when BatchJob started running end_time DateTime when BatchJob ended Every workload manager is different and there are numerous job states intentionally not considered in the BatchJob model, including starting , exiting , user_hold , dep_hold , etc. It is the responsibility of the site's Scheduler interface to translate real scheduler states to one of the few coarse-grained Balsam BatchJob states: queued , running , or finished . stateDiagram-v2 pending_submission --> queued pending_submission --> submit_failed queued --> running running --> finished pending_submission --> pending_deletion queued --> pending_deletion running --> pending_deletion pending_deletion --> finished","title":"BatchJob"},{"location":"development/data-model/#session","text":"Field Name Description id Unique ID heartbeat DateTime of last session tick API call batch_job Non-nullable ForeignKey to BatchJob this Session is running under Session creation only requires providing batch_job_id . Session tick has empty payload Session acquire endpoint uses a special JobAcquireSerializer representation: Field Description states list of states to acquire max_num_acquire limit number of jobs to acquire filter_tags filter Jobs for which job.tags contains all {tag_name: tag_value} pairs node_resources Nested NodeResource representation placing resource constraints on what Jobs may be acquired order_by order returned jobs according to a set of Job fields (may include ascending or descending num_nodes , node_packing_count , wall_time_min ) The nested NodeResource representation is provided as a dict with the structure: { \"max_jobs_per_node\" : 1 , # Determined by Site settings for each Launcher job mode \"max_wall_time_min\" : 60 , \"running_job_counts\" : [ 0 , 1 , 0 ], \"node_occupancies\" : [ 0.0 , 1.0 , 0.0 ], \"idle_cores\" : [ 64 , 63 , 64 ], \"idle_gpus\" : [ 1 , 0 , 1 ], }","title":"Session"},{"location":"development/data-model/#transferitem","text":"Field Name Description id Unique TransferItem ID job ForeignKey to Job protocol globus or rsync direction in or out . If in , the transfer is from remote_netloc:source_path to Job.workdir/destination_path . If out , the transfer is from Job.workdir/src_path to remote_netloc:dest_path . remote_netloc The Globus endpoint UUID or user@hostname of the remote data location source_path If stage- in : the remote path. If stage- out : the local path destination_path If stage- in : the local path. If stage- out : the remote path. state pending -> active -> done or error task_id Unique identifier of the Transfer task (e.g. Globus Task UUID) transfer_info JSONField for Error messages, average bandwidth, transfer time, etc... There is no create ( POST ) method on the /transfers endpoint, because TransferItem creation is directly linked with Job creation. The related Transfers are nested in the Job representation when POSTING new jobs. The following fields are fixed at creation time: id job protocol direction remote_netloc source_path dest_path For list (GET), the representation includes all fields. job_id represents the Job by primary key. For update (PUT and PATCH), only state , task_id , and transfer_info may be modified. The update of a state to done triggers a check of the related Job 's transfers to determine whether the job can be advanced to STAGED_IN .","title":"TransferItem"},{"location":"development/data-model/#logevent","text":"Field Name Description id Unique ID job ForeignKey to Job undergoing event timestamp DateTime of event from_state Job state before transition to_state Job state after transition data JSONField containing {message: str} and other optional data For transitions to or from RUNNING , the data includes nodes as a fractional number of occupied nodes. This enables clients to generate throughput and utilization views without having to fetch entire related Jobs. This is a read only-API with all fields included. The related Job is represented by primary key job_id field.","title":"LogEvent"},{"location":"development/deploy/","text":"Installing and running the API server Using Docker-Compose The simplest way to run a test server instance is with Docker Compose . Run this command in the root balsam/ directory: docker-compose up -d This will start the API server on localhost:8000 along with Postgres and Redis. You can alter the port and other server configuration by changing environment variables in the docker-compose.yml file. Using Singularity An analogous deployment for systems running Singularity is singularity-deploy.sh . You may configure the server environment in this script and launch the API containers with: source singularity-deploy.sh Using your host OS A bare metal deployment requires PostgreSQL (Redis is optional). If these requiremenst are in place, you can create a new server instance with: balsam server deploy -p /path/to/server-dir This will create a new directory server-dir housing the database and log files. You may start and stop the database with balsam server up and balsam server down , respectively. Install Postgres If which pg_ctl does not show a Postgres on your system, get the Postgres binaries . You only need to unzip and add the postgres bin/ to your PATH. Install Redis (Optional) Inside your virtualenv, run the redis-install.sh script to install Redis (or DIY). This will copy the built Redis binary in your virtualenv bin/ .","title":"Running a Balsam Server"},{"location":"development/deploy/#installing-and-running-the-api-server","text":"","title":"Installing and running the API server"},{"location":"development/deploy/#using-docker-compose","text":"The simplest way to run a test server instance is with Docker Compose . Run this command in the root balsam/ directory: docker-compose up -d This will start the API server on localhost:8000 along with Postgres and Redis. You can alter the port and other server configuration by changing environment variables in the docker-compose.yml file.","title":"Using Docker-Compose"},{"location":"development/deploy/#using-singularity","text":"An analogous deployment for systems running Singularity is singularity-deploy.sh . You may configure the server environment in this script and launch the API containers with: source singularity-deploy.sh","title":"Using Singularity"},{"location":"development/deploy/#using-your-host-os","text":"A bare metal deployment requires PostgreSQL (Redis is optional). If these requiremenst are in place, you can create a new server instance with: balsam server deploy -p /path/to/server-dir This will create a new directory server-dir housing the database and log files. You may start and stop the database with balsam server up and balsam server down , respectively.","title":"Using your host OS"},{"location":"development/deploy/#install-postgres","text":"If which pg_ctl does not show a Postgres on your system, get the Postgres binaries . You only need to unzip and add the postgres bin/ to your PATH.","title":"Install Postgres"},{"location":"development/deploy/#install-redis-optional","text":"Inside your virtualenv, run the redis-install.sh script to install Redis (or DIY). This will copy the built Redis binary in your virtualenv bin/ .","title":"Install Redis (Optional)"},{"location":"development/jobsource/","text":"Job Sources & Status Updaters Job Source runnable_count() almost_runnable_count() Status Updater A queue & thread for pooling Job status updates","title":"JobSource and StatusUpdater"},{"location":"development/jobsource/#job-sources-status-updaters","text":"","title":"Job Sources &amp; Status Updaters"},{"location":"development/jobsource/#job-source","text":"runnable_count() almost_runnable_count()","title":"Job Source"},{"location":"development/jobsource/#status-updater","text":"A queue & thread for pooling Job status updates","title":"Status Updater"},{"location":"development/layout/","text":"Project Layout This pages gives an overview of the Balsam subpackages, important components therein, and their relationships to each other. balsam.server This subpackage is the fully self-contained codebase for the API server, implemented with FastAPI and SQLAlchemy . server.main defines the top-level URL routes into views located in balsam.server.routers balsam.server.routers defines the possible API actions balsam.server.models encapsulates the database and any actions that involve database communication. balsam.client This package defines the RESTClient interface which forms the basis for all API interactions from the rest of Balsam. The implementations capture the details of authentication to the Balsam API and performing HTTP requests. balsam._api Whereas the RESTClient provides a lower-level interface rooted in exchanging JSON data over HTTP, the balsam._api defines Django ORM-like Models and Managers to emulate the original Balsam API: from balsam.config import SiteConfig conf = SiteConfig () current_site_id = conf . site_id App = conf . client . App my_apps = App . objects . filter ( site_id = current_site_id ) This user-facing API is accessed via the client (a RESTClient instance) located in the SiteConfig . Rather than using the RESTClient directly, users obtain handles to the model classes: client.Site client.App client.Job client.BatchJob client.TransferItem client.EventLog Each of these Model classes subclasses the BalsamModel base class. The permitted model fields and their types are auto-generated from the schema in balsam.schemas (see below). A Manager instance named objects on each Model class is used to build queries and access the API via the client . balsam.schemas Pydantic is used to define the data structures of the above models and perform validation on the data. The schemas under balsam.schemas are used both by the user-facing balsam._api classes and the backend balsam.server.routers API. Thus when an update to the schema is made, both the client and server-side code inherit the change. balsam.platform The platform subpackage contains all the platform-specific interfaces to various HPC systems. The goal of this architecture is to make porting Balsam to new HPC systems easier: a developer should only have to write minimal interface code under balsam.platform and add an appropriate default configuration under balsam.config.defaults . AppRun This is the Balsam-wide application launch interface which is used by the Balsam launcher (pilot job) components. It encapsulates the full lifecycle of running a shell command on a system: Setting up environment Setting working directory and output file Specifying compute resources Running command and monitoring the process Facilities to terminate process/check output/return code AppRun implementations may use a subprocess to manage an mpirun (or equivalent) command, run a local subprocess, or do something entirely different that does not involve a subprocess at all. ComputeNode The Balsam launcher uses this interface to discover available compute resources within a batch job, as well as to enumerate resources (CPU cores, GPUs) on a node and track their occupancy. Scheduler The Balsam Site uses this interface to interact with the local resource manager (e.g. Slurm, Cobalt) to submit new batch jobs, check on job statuses, and inspect other system-wide metrics (e.g. backfill availability). TransferInterface The Balsam Site uses this interface to submit new transfer tasks and poll on their status. A GlobusTransfer interface is implemented for batching Job stage-ins/stage-outs into Globus Transfer tasks. balsam.config Unlike the previous Balsam, where a minimal global configuration was stored in the home directory ( ~/.balsam ), here a comprehensive YAML configuration file is stored for each Balsam Site in the Site directory as settings.yml . This improves isolation when multiple systems share a home file system, and enables flexible configuration of Site behavior per-project . The Settings are also described by a Pydantic schema, which is used to validate the YAML file every time it is loaded. The loaded settings are stored in a SiteConfig instance, which provides a handle to many related Site-level entities (e.g. the client , the Job working directories) and administrative functions (e.g. bootstrapping a new Site ). balsam.site This subpackage contains the real functional core of Balsam: the various components that run on a Site to execute workflows. JobSource Launchers and pre/post-processing modules use this interface to fetch Jobs from the API. The abstraction keeps specific API calls out of the launcher code base, and permits different implementation strategies: FixedDepthJobSource maintains a queue of pre-fetched jobs using a background process SynchronousJobSource performs a blocking API call to fetch jobs according to a specification of available resources. StatusUpdater The StatusUpdater interface is used to manage job status updates, and also helps to keep API-specific code out of the other Balsam internals. The primary implementation BulkStatusUpdater pools update events that are passed via queue to a background process, and performs bulk API updates to reduce the frequency of API calls. ScriptTemplate The ScriptTemplate is used to generate shell scripts for submission to the local resource manager, using a Site-specific job template file. ApplicationDefinition Users write their own subclasses of ApplicationDefinition to configure the Apps that may run at a particular Balsam Site. These classes are written into Python modules in the Site apps/ folder. Each ApplicationDefinition is automatically synced with the API when users run the balsam app sync command. Launcher The MPI and serial job modes of the Balsam launcher are implemented here. These are standalone, executable Python scripts that carry out the execution of Balsam Jobs (sometimes called a pilot job mechanism). The launchers are invoked from a shell script generated by the ScriptTemplate which is submitted to the local resource manager (via the Scheduler interface). balsam.site.service The Balsam Site daemon comprises a group of background processes that run on behalf of the user. The daemon may run on a login node, or on any other resource appropriate for a long-running background process. The only requirements are that: The Site daemon can access the filesystem with the Site directory, and The Site daemon can access the local resource manager (e.g. perform qsub ) The Site daemon can access the Balsam API The Site daemon is organized as a collection of BalsamService classes, each of which describes a particular background process. This setup is highly modular: users can easily configure which service modules are in use, and developers can implement additional services that hook directly into the Site. SchedulerService This BalsamService component syncs with BatchJobs in the Balsam API and uses the Scheduler platform interface to submit new BatchJobs and update the status of existing BatchJobs . It does not automate the process of job submission -- it only serves to keep the API state and local resource manager state synchronized. For example, a user performing the balsam submit-launch command causes a new BatchJob to be created in the API. The SchedulerService then detects this new BatchJob , generates an appropriate script from the ScriptTemplate , and submits it to the local Slurm scheduler. AutoscaleService This BalsamService monitors the backlog of Jobs and locally available compute resources, and it automatically submits new BatchJobs to the API to adapt to realtime workloads. This is a form of automated job submission, which works together with the SchedulerService to fully automate resource allocation and execution. QueueMaintainerService This is another, simpler, form of automated job submission, in which a constant number of fixed-size BatchJobs are maintained at a Site (e.g. keep 5 jobs queued at all times). Intended to get through a long campaign of runs. ProcessingService This service carries out the execution of various workflow steps that are defined on the ApplicationDefinition : preprocess() postprocess() handle_error() handle_timeout() These are meant to be lightweight and IO-bound tasks that run in a process pool on the login node or similar resource. Compute-intensive tasks should be performed in the main body of an App. TransferService This service automates staging in data from remote locations prior to the preprocess() step of a Job, and staging results out to other remote locations after postprocess() . The service batches files and directories that are to be moved between a certain pair of endpoints, and creates batch Transfer tasks via the TransferInterface . balsam.cmdline The command line interfaces to Balsam are written as Python functions decorated with Click","title":"Project Layout"},{"location":"development/layout/#project-layout","text":"This pages gives an overview of the Balsam subpackages, important components therein, and their relationships to each other.","title":"Project Layout"},{"location":"development/layout/#balsamserver","text":"This subpackage is the fully self-contained codebase for the API server, implemented with FastAPI and SQLAlchemy . server.main defines the top-level URL routes into views located in balsam.server.routers balsam.server.routers defines the possible API actions balsam.server.models encapsulates the database and any actions that involve database communication.","title":"balsam.server"},{"location":"development/layout/#balsamclient","text":"This package defines the RESTClient interface which forms the basis for all API interactions from the rest of Balsam. The implementations capture the details of authentication to the Balsam API and performing HTTP requests.","title":"balsam.client"},{"location":"development/layout/#balsam_api","text":"Whereas the RESTClient provides a lower-level interface rooted in exchanging JSON data over HTTP, the balsam._api defines Django ORM-like Models and Managers to emulate the original Balsam API: from balsam.config import SiteConfig conf = SiteConfig () current_site_id = conf . site_id App = conf . client . App my_apps = App . objects . filter ( site_id = current_site_id ) This user-facing API is accessed via the client (a RESTClient instance) located in the SiteConfig . Rather than using the RESTClient directly, users obtain handles to the model classes: client.Site client.App client.Job client.BatchJob client.TransferItem client.EventLog Each of these Model classes subclasses the BalsamModel base class. The permitted model fields and their types are auto-generated from the schema in balsam.schemas (see below). A Manager instance named objects on each Model class is used to build queries and access the API via the client .","title":"balsam._api"},{"location":"development/layout/#balsamschemas","text":"Pydantic is used to define the data structures of the above models and perform validation on the data. The schemas under balsam.schemas are used both by the user-facing balsam._api classes and the backend balsam.server.routers API. Thus when an update to the schema is made, both the client and server-side code inherit the change.","title":"balsam.schemas"},{"location":"development/layout/#balsamplatform","text":"The platform subpackage contains all the platform-specific interfaces to various HPC systems. The goal of this architecture is to make porting Balsam to new HPC systems easier: a developer should only have to write minimal interface code under balsam.platform and add an appropriate default configuration under balsam.config.defaults .","title":"balsam.platform"},{"location":"development/layout/#apprun","text":"This is the Balsam-wide application launch interface which is used by the Balsam launcher (pilot job) components. It encapsulates the full lifecycle of running a shell command on a system: Setting up environment Setting working directory and output file Specifying compute resources Running command and monitoring the process Facilities to terminate process/check output/return code AppRun implementations may use a subprocess to manage an mpirun (or equivalent) command, run a local subprocess, or do something entirely different that does not involve a subprocess at all.","title":"AppRun"},{"location":"development/layout/#computenode","text":"The Balsam launcher uses this interface to discover available compute resources within a batch job, as well as to enumerate resources (CPU cores, GPUs) on a node and track their occupancy.","title":"ComputeNode"},{"location":"development/layout/#scheduler","text":"The Balsam Site uses this interface to interact with the local resource manager (e.g. Slurm, Cobalt) to submit new batch jobs, check on job statuses, and inspect other system-wide metrics (e.g. backfill availability).","title":"Scheduler"},{"location":"development/layout/#transferinterface","text":"The Balsam Site uses this interface to submit new transfer tasks and poll on their status. A GlobusTransfer interface is implemented for batching Job stage-ins/stage-outs into Globus Transfer tasks.","title":"TransferInterface"},{"location":"development/layout/#balsamconfig","text":"Unlike the previous Balsam, where a minimal global configuration was stored in the home directory ( ~/.balsam ), here a comprehensive YAML configuration file is stored for each Balsam Site in the Site directory as settings.yml . This improves isolation when multiple systems share a home file system, and enables flexible configuration of Site behavior per-project . The Settings are also described by a Pydantic schema, which is used to validate the YAML file every time it is loaded. The loaded settings are stored in a SiteConfig instance, which provides a handle to many related Site-level entities (e.g. the client , the Job working directories) and administrative functions (e.g. bootstrapping a new Site ).","title":"balsam.config"},{"location":"development/layout/#balsamsite","text":"This subpackage contains the real functional core of Balsam: the various components that run on a Site to execute workflows.","title":"balsam.site"},{"location":"development/layout/#jobsource","text":"Launchers and pre/post-processing modules use this interface to fetch Jobs from the API. The abstraction keeps specific API calls out of the launcher code base, and permits different implementation strategies: FixedDepthJobSource maintains a queue of pre-fetched jobs using a background process SynchronousJobSource performs a blocking API call to fetch jobs according to a specification of available resources.","title":"JobSource"},{"location":"development/layout/#statusupdater","text":"The StatusUpdater interface is used to manage job status updates, and also helps to keep API-specific code out of the other Balsam internals. The primary implementation BulkStatusUpdater pools update events that are passed via queue to a background process, and performs bulk API updates to reduce the frequency of API calls.","title":"StatusUpdater"},{"location":"development/layout/#scripttemplate","text":"The ScriptTemplate is used to generate shell scripts for submission to the local resource manager, using a Site-specific job template file.","title":"ScriptTemplate"},{"location":"development/layout/#applicationdefinition","text":"Users write their own subclasses of ApplicationDefinition to configure the Apps that may run at a particular Balsam Site. These classes are written into Python modules in the Site apps/ folder. Each ApplicationDefinition is automatically synced with the API when users run the balsam app sync command.","title":"ApplicationDefinition"},{"location":"development/layout/#launcher","text":"The MPI and serial job modes of the Balsam launcher are implemented here. These are standalone, executable Python scripts that carry out the execution of Balsam Jobs (sometimes called a pilot job mechanism). The launchers are invoked from a shell script generated by the ScriptTemplate which is submitted to the local resource manager (via the Scheduler interface).","title":"Launcher"},{"location":"development/layout/#balsamsiteservice","text":"The Balsam Site daemon comprises a group of background processes that run on behalf of the user. The daemon may run on a login node, or on any other resource appropriate for a long-running background process. The only requirements are that: The Site daemon can access the filesystem with the Site directory, and The Site daemon can access the local resource manager (e.g. perform qsub ) The Site daemon can access the Balsam API The Site daemon is organized as a collection of BalsamService classes, each of which describes a particular background process. This setup is highly modular: users can easily configure which service modules are in use, and developers can implement additional services that hook directly into the Site.","title":"balsam.site.service"},{"location":"development/layout/#schedulerservice","text":"This BalsamService component syncs with BatchJobs in the Balsam API and uses the Scheduler platform interface to submit new BatchJobs and update the status of existing BatchJobs . It does not automate the process of job submission -- it only serves to keep the API state and local resource manager state synchronized. For example, a user performing the balsam submit-launch command causes a new BatchJob to be created in the API. The SchedulerService then detects this new BatchJob , generates an appropriate script from the ScriptTemplate , and submits it to the local Slurm scheduler.","title":"SchedulerService"},{"location":"development/layout/#autoscaleservice","text":"This BalsamService monitors the backlog of Jobs and locally available compute resources, and it automatically submits new BatchJobs to the API to adapt to realtime workloads. This is a form of automated job submission, which works together with the SchedulerService to fully automate resource allocation and execution.","title":"AutoscaleService"},{"location":"development/layout/#queuemaintainerservice","text":"This is another, simpler, form of automated job submission, in which a constant number of fixed-size BatchJobs are maintained at a Site (e.g. keep 5 jobs queued at all times). Intended to get through a long campaign of runs.","title":"QueueMaintainerService"},{"location":"development/layout/#processingservice","text":"This service carries out the execution of various workflow steps that are defined on the ApplicationDefinition : preprocess() postprocess() handle_error() handle_timeout() These are meant to be lightweight and IO-bound tasks that run in a process pool on the login node or similar resource. Compute-intensive tasks should be performed in the main body of an App.","title":"ProcessingService"},{"location":"development/layout/#transferservice","text":"This service automates staging in data from remote locations prior to the preprocess() step of a Job, and staging results out to other remote locations after postprocess() . The service batches files and directories that are to be moved between a certain pair of endpoints, and creates batch Transfer tasks via the TransferInterface .","title":"TransferService"},{"location":"development/layout/#balsamcmdline","text":"The command line interfaces to Balsam are written as Python functions decorated with Click","title":"balsam.cmdline"},{"location":"development/service/","text":"The Service Modules Service module interface Service Modules must be split into separate Python module files under the balsam.service subpackage. Each service module has a top-level run() function in the module scope. This function must accept keyword arguments corresponding to the service module configuration keys. This configuration: service_modules : batch_job_manager : scheduler_interface : balsam.platform.scheduler.CobaltThetaScheduler poll_interval_sec : 30 allowed_projects : - datascience default_project : datascience allowed_queues : - name : default max_nodes : 4096 max_walltime : 24h max_queued : 20 - name : debug_cache_quad max_nodes : 8 max_walltime : 1h max_queued : 1 results in creation of a multiprocessing.Process instance like: Process ( target = batch_job_manager . run , kwargs = dict ( scheduler_interface = SchedulerClass , poll_interval_sec = 30 , allowed_projects = allowed_projects_list , default_project = \"datascience\" , allowed_queues = allowed_queues_list ) ) Balsam service startup process User invokes balsam service start in a Site directory Concurrent startups are protected by PID file CLI loads local & global config Sets up global logging config Gets list of service modules & their configuration from component factory Launches module.run() Process for each Polls status, restarts as necessary BatchJobManager Interfaces to scheduler, performs job submission and syncs qstat state with Balsam API. Does not decide when to submit more jobs -- that is the responsibility of AutoScaler BatchJob sync protocol: GET list of currently active batchjobs (filter by Site, exclude inactive) Perform local qstat For each active BatchJob , compare with the qstat record and take action: pending_submission : perform qsub , update scheduler_id and set state=\"queued\" on submission error, set status_message=error and state=\"submit-failed\" pending_deletion if in qstat list: qdel (leave it as \"pending-deletion\" until its actually gone) if not in qstat list (i.e. job cleanly exited or it never ran), set state=finished and parse logs to set start_time and end_time On any status change, update state If a job finished for any reason, parse the logs to update start_time and end_time If any parameters like walltime were changed, run qalter if possible. If it's too late; revert the change Apply bulk-update via PATCH on list view For any running job, add revert=True to the patch dict. This ensures that the running job parameters are consistent with the scheduler state Site sync Periodically update Site nodelist, state of all job queues AutoScaler Monitors Job backlog, currently queued BatchJobs, currently available resources. Decides when a new queue submission is necessary and posts a new BatchJob object to the API. Modes: Fixed submissions (keep N jobs queued at all times) Backfill sniper Count running, preprocessed, staged_in jobs backlog = staged_in + preprocessed Get all backfill windows submit based on: minimum job submission heuristic available backfill windows aggregate node requirement of runnable jobs number of currently available nodes (from running BatchJobs) number of expected-soon-to-be-available nodes (recently queued in backfill) deletes BatchJobs that did not go through within X minutes Processing Workdir existence is ensured/double-checked before Stage-in, Preproc, Run Runs pre- and post-processing scripts Multiprocessing pool with shared JobSource and StatusUpdater No need to acquire locks since there is no race Fetch staged_in, done, error, timeout jobs","title":"Service modules"},{"location":"development/service/#the-service-modules","text":"","title":"The Service Modules"},{"location":"development/service/#service-module-interface","text":"Service Modules must be split into separate Python module files under the balsam.service subpackage. Each service module has a top-level run() function in the module scope. This function must accept keyword arguments corresponding to the service module configuration keys. This configuration: service_modules : batch_job_manager : scheduler_interface : balsam.platform.scheduler.CobaltThetaScheduler poll_interval_sec : 30 allowed_projects : - datascience default_project : datascience allowed_queues : - name : default max_nodes : 4096 max_walltime : 24h max_queued : 20 - name : debug_cache_quad max_nodes : 8 max_walltime : 1h max_queued : 1 results in creation of a multiprocessing.Process instance like: Process ( target = batch_job_manager . run , kwargs = dict ( scheduler_interface = SchedulerClass , poll_interval_sec = 30 , allowed_projects = allowed_projects_list , default_project = \"datascience\" , allowed_queues = allowed_queues_list ) )","title":"Service module interface"},{"location":"development/service/#balsam-service-startup-process","text":"User invokes balsam service start in a Site directory Concurrent startups are protected by PID file CLI loads local & global config Sets up global logging config Gets list of service modules & their configuration from component factory Launches module.run() Process for each Polls status, restarts as necessary","title":"Balsam service startup process"},{"location":"development/service/#batchjobmanager","text":"Interfaces to scheduler, performs job submission and syncs qstat state with Balsam API. Does not decide when to submit more jobs -- that is the responsibility of AutoScaler","title":"BatchJobManager"},{"location":"development/service/#batchjob-sync-protocol","text":"GET list of currently active batchjobs (filter by Site, exclude inactive) Perform local qstat For each active BatchJob , compare with the qstat record and take action: pending_submission : perform qsub , update scheduler_id and set state=\"queued\" on submission error, set status_message=error and state=\"submit-failed\" pending_deletion if in qstat list: qdel (leave it as \"pending-deletion\" until its actually gone) if not in qstat list (i.e. job cleanly exited or it never ran), set state=finished and parse logs to set start_time and end_time On any status change, update state If a job finished for any reason, parse the logs to update start_time and end_time If any parameters like walltime were changed, run qalter if possible. If it's too late; revert the change Apply bulk-update via PATCH on list view For any running job, add revert=True to the patch dict. This ensures that the running job parameters are consistent with the scheduler state","title":"BatchJob sync protocol:"},{"location":"development/service/#site-sync","text":"Periodically update Site nodelist, state of all job queues","title":"Site sync"},{"location":"development/service/#autoscaler","text":"Monitors Job backlog, currently queued BatchJobs, currently available resources. Decides when a new queue submission is necessary and posts a new BatchJob object to the API. Modes: Fixed submissions (keep N jobs queued at all times) Backfill sniper Count running, preprocessed, staged_in jobs backlog = staged_in + preprocessed Get all backfill windows submit based on: minimum job submission heuristic available backfill windows aggregate node requirement of runnable jobs number of currently available nodes (from running BatchJobs) number of expected-soon-to-be-available nodes (recently queued in backfill) deletes BatchJobs that did not go through within X minutes","title":"AutoScaler"},{"location":"development/service/#processing","text":"Workdir existence is ensured/double-checked before Stage-in, Preproc, Run Runs pre- and post-processing scripts Multiprocessing pool with shared JobSource and StatusUpdater No need to acquire locks since there is no race Fetch staged_in, done, error, timeout jobs","title":"Processing"},{"location":"development/site-config/","text":"Site Configuration The API Client configuration The balsam login command updates API client configuration data in the ~/.balsam/client.yml file. This file contains a reference to a Client class and the configuration needed to instantiate a Client. All components of Balsam using the API then read from the client settings file upon initialization. The Site configuration The behavior of each Balsam Site is largely controlled by the settings.yml file located in the Site directory. Upon initializing a new Site with the balsam site init command, a user is prompted to select from one of the pre-packaged default configurations. Suitable example files are then copied into the settings.yml and batchjob.tmpl files. Additionally, pre-configured Balsam application modules are copied into the apps/ subdirectory. Several fields in the Site configuration file or Job Template are sync'ed with the Site API: allowed_projects lets the API know about projects that the Site may submit to allowed_queues lets the API know about the local queue policy transfer_locations lets the API know about remote Globus endpoints or scp addresses that the Site is willing to stage data in/out from globus_endpoint_id sets a local Globus endpoint ID for the Site optional_batch_job_params lets the API know about \"pass-through\" parameters that the Job template will accept in submitting a BatchJob Local configuration A Balsam Site is initialized with balsam init . Site settings for each launcher Job mode dictate whether multi-apps-per-node is supported and what is the max occupancy per node. Global configuration","title":"Site Configuration"},{"location":"development/site-config/#site-configuration","text":"","title":"Site Configuration"},{"location":"development/site-config/#the-api-client-configuration","text":"The balsam login command updates API client configuration data in the ~/.balsam/client.yml file. This file contains a reference to a Client class and the configuration needed to instantiate a Client. All components of Balsam using the API then read from the client settings file upon initialization.","title":"The API Client configuration"},{"location":"development/site-config/#the-site-configuration","text":"The behavior of each Balsam Site is largely controlled by the settings.yml file located in the Site directory. Upon initializing a new Site with the balsam site init command, a user is prompted to select from one of the pre-packaged default configurations. Suitable example files are then copied into the settings.yml and batchjob.tmpl files. Additionally, pre-configured Balsam application modules are copied into the apps/ subdirectory. Several fields in the Site configuration file or Job Template are sync'ed with the Site API: allowed_projects lets the API know about projects that the Site may submit to allowed_queues lets the API know about the local queue policy transfer_locations lets the API know about remote Globus endpoints or scp addresses that the Site is willing to stage data in/out from globus_endpoint_id sets a local Globus endpoint ID for the Site optional_batch_job_params lets the API know about \"pass-through\" parameters that the Job template will accept in submitting a BatchJob","title":"The Site configuration"},{"location":"development/site-config/#local-configuration","text":"A Balsam Site is initialized with balsam init . Site settings for each launcher Job mode dictate whether multi-apps-per-node is supported and what is the max occupancy per node.","title":"Local configuration"},{"location":"development/site-config/#global-configuration","text":"","title":"Global configuration"},{"location":"development/todo/","text":"Roadmap Higher priority Generalized AutoScaler implementation Server & Client Auth: beyond password-based Platform Unit Tests & Test runner framework (ability to run tests selectively based on location) Site unit tests & test framework: Processing Transfer Scheduler Launcher (MPI mode) Launcher (Serial mode) Integration test Throughput benchmark Medium priority Polish CLI: old balsam-style balsam ls tabular views API \"DAG\" capability: creating prototype dags and instances of dags Type annotations in API DB query profiling & optimization: prefetch loads & bulk writes Web Interface Nuxt / Vuetify Web UI project skeleton Wire up Basic Auth with Axios+VueX using VueX:websocket plugin for live Job Status updates Set up VueX store and actions Sites view (grid layout of Site cards) Apps view BatchJobs view (list) BatchJobs view (detail: show related Jobs & utilization plot for the Job) Jobs datatable Interactive Job creation form Transfers view History view: graphical filter controls: by Site, by Tags, by Date Range show: throughput, utilization, available nodes Jupyter Interface Jupyter API wrappers Figure out how to add useful signatures to Model, Manager APIs Explicit __init__ on models and Jupyter notebook introspection-friendly API CI black, flake8, mypy Server tests Client API tests Platform unit tests Site unit tests Integration test Download Install in venv Register & login to known host:port Create a Site Populate Apps and Jobs Activate Globus endpoints Configure settings to run a queue_maintainer with depth of 1 Run service Monitor logs, job and batchjob states report failures in any step report ERROR from log report failed state in API transmit logs, state history, job outputs as artifacts Test that transfers actually occurred","title":"Roadmap"},{"location":"development/todo/#roadmap","text":"","title":"Roadmap"},{"location":"development/todo/#higher-priority","text":"Generalized AutoScaler implementation Server & Client Auth: beyond password-based Platform Unit Tests & Test runner framework (ability to run tests selectively based on location) Site unit tests & test framework: Processing Transfer Scheduler Launcher (MPI mode) Launcher (Serial mode) Integration test Throughput benchmark","title":"Higher priority"},{"location":"development/todo/#medium-priority","text":"Polish CLI: old balsam-style balsam ls tabular views API \"DAG\" capability: creating prototype dags and instances of dags Type annotations in API DB query profiling & optimization: prefetch loads & bulk writes","title":"Medium priority"},{"location":"development/todo/#web-interface","text":"Nuxt / Vuetify Web UI project skeleton Wire up Basic Auth with Axios+VueX using VueX:websocket plugin for live Job Status updates Set up VueX store and actions Sites view (grid layout of Site cards) Apps view BatchJobs view (list) BatchJobs view (detail: show related Jobs & utilization plot for the Job) Jobs datatable Interactive Job creation form Transfers view History view: graphical filter controls: by Site, by Tags, by Date Range show: throughput, utilization, available nodes","title":"Web Interface"},{"location":"development/todo/#jupyter-interface","text":"Jupyter API wrappers Figure out how to add useful signatures to Model, Manager APIs Explicit __init__ on models and Jupyter notebook introspection-friendly API","title":"Jupyter Interface"},{"location":"development/todo/#ci","text":"black, flake8, mypy Server tests Client API tests Platform unit tests Site unit tests Integration test Download Install in venv Register & login to known host:port Create a Site Populate Apps and Jobs Activate Globus endpoints Configure settings to run a queue_maintainer with depth of 1 Run service Monitor logs, job and batchjob states report failures in any step report ERROR from log report failed state in API transmit logs, state history, job outputs as artifacts Test that transfers actually occurred","title":"CI"},{"location":"development/web-client/","text":"Web Interface Deployment Serve static content (Nuxt build) with NGINX Reverse proxy /api/v1/ to Gunicorn Gunicorn runs Uvicorn workers gunicorn balsam:asgi_app -k uvicorn.workers.UvicornWorker Support HTTP keepalive & websockets Run Redis on localhost and use RedisChannelLayer Channels deployment AsyncWebsocketConsumers on connect() and disconnect() , add/remove self from balsam_web_user_{id} group model/manager code calls channel_layer.group_send(\"balsam_web_user_{id}\", {type: \"jobs.update\", payload}) Consumers push these events onto websocket; VueX plugin carries out mutations asynchronously Updates contain the full instance, not just a patch. That way, an update can be have \"upsert\" semantics on the client store (update or create) Resource Event Type Site Create, Update, Delete App Create, Update, Delete BatchJob Create, Update, Delete Session Create, Update, Delete Job CreateList, UpdateList, DeleteList TransferItem CreateList, UpdateList, DeleteList LogEvent CreateList, UpdateList, DeleteList Client data flow API actions do not clobber/overwrite the store; they either update existing items or create. Only Websocket \"delete\" events cause deletion. graph LR actions(VueX Actions) -- mutate --> store[(VueX Store)] websocket(Websocket Plugin) -- mutate --> store store -- getters --> view(Vue Component) wsapi(WebSocket API) -- subscribe --> websocket axios(Axios API) -- fetch --> actions style wsapi fill:#eea style axios fill:#eea Client establishes Websocket connection first Websocket updates start to come in asynchronously Client issues initial API actions to fetch and update store: current user info all Sites all Apps all Sessions last 100 BatchJobs last 100 Jobs Site-wise state counts Any subsequent API actions are for lazy-loading Jobs , BatchJobs , Transfers , or LogEvents : JobDataTableView HistoryView or BatchJobDetail view TransferView Every GET API action (changing filters, ordering, page number, loading a new view) does not evict stale data. It should only insert/update fetched data. Thus, the Store might contain a lot more data than the current view is requesting. For instance, we might be looking at Jobs that ran in BatchJob a month ago, while 10k Jobs that ran in the last hour are still in the store and receiving ongoing Websocket updates. Vue components use either computed properties or Store Getters to get the ordered subset of data in the rendered view. Note As long as the Websocket connection remains alive, GET requests shall not be repeated on the web client side. Any changes would have already been recorded via the Websocket plugin. We don't need to cache requests; we just remember the list of requests to avoid repeating. If the Websocket connection is interrupted, we can clear the list of past GET requests to refresh data that may have been lost in the interim.","title":"Web Client"},{"location":"development/web-client/#web-interface","text":"","title":"Web Interface"},{"location":"development/web-client/#deployment","text":"Serve static content (Nuxt build) with NGINX Reverse proxy /api/v1/ to Gunicorn Gunicorn runs Uvicorn workers gunicorn balsam:asgi_app -k uvicorn.workers.UvicornWorker Support HTTP keepalive & websockets Run Redis on localhost and use RedisChannelLayer Channels deployment","title":"Deployment"},{"location":"development/web-client/#asyncwebsocketconsumers","text":"on connect() and disconnect() , add/remove self from balsam_web_user_{id} group model/manager code calls channel_layer.group_send(\"balsam_web_user_{id}\", {type: \"jobs.update\", payload}) Consumers push these events onto websocket; VueX plugin carries out mutations asynchronously Updates contain the full instance, not just a patch. That way, an update can be have \"upsert\" semantics on the client store (update or create) Resource Event Type Site Create, Update, Delete App Create, Update, Delete BatchJob Create, Update, Delete Session Create, Update, Delete Job CreateList, UpdateList, DeleteList TransferItem CreateList, UpdateList, DeleteList LogEvent CreateList, UpdateList, DeleteList","title":"AsyncWebsocketConsumers"},{"location":"development/web-client/#client-data-flow","text":"API actions do not clobber/overwrite the store; they either update existing items or create. Only Websocket \"delete\" events cause deletion. graph LR actions(VueX Actions) -- mutate --> store[(VueX Store)] websocket(Websocket Plugin) -- mutate --> store store -- getters --> view(Vue Component) wsapi(WebSocket API) -- subscribe --> websocket axios(Axios API) -- fetch --> actions style wsapi fill:#eea style axios fill:#eea Client establishes Websocket connection first Websocket updates start to come in asynchronously Client issues initial API actions to fetch and update store: current user info all Sites all Apps all Sessions last 100 BatchJobs last 100 Jobs Site-wise state counts Any subsequent API actions are for lazy-loading Jobs , BatchJobs , Transfers , or LogEvents : JobDataTableView HistoryView or BatchJobDetail view TransferView Every GET API action (changing filters, ordering, page number, loading a new view) does not evict stale data. It should only insert/update fetched data. Thus, the Store might contain a lot more data than the current view is requesting. For instance, we might be looking at Jobs that ran in BatchJob a month ago, while 10k Jobs that ran in the last hour are still in the store and receiving ongoing Websocket updates. Vue components use either computed properties or Store Getters to get the ordered subset of data in the rendered view. Note As long as the Websocket connection remains alive, GET requests shall not be repeated on the web client side. Any changes would have already been recorded via the Websocket plugin. We don't need to cache requests; we just remember the list of requests to avoid repeating. If the Websocket connection is interrupted, we can clear the list of past GET requests to refresh data that may have been lost in the interim.","title":"Client data flow"},{"location":"tutorials/theta-quickstart/","text":"Running on Theta Installation First create a new virtualenv and install it on Balsam: $ /soft/datascience/create_env.sh my-env # Or do it yourself $ source my-env/bin/activate $ git clone https://github.com/balsam-alcf/balsam.git $ cd balsam/ $ git checkout develop $ pip install -e . Log In Now you will need to create a Balsam user account and log in. Logging in fetches an access token that is used to identify you in subsequent API interactions, until the token expires and you have to log in again. $ balsam register Balsam server address: http://generic-01:8000 # Set your own username and password # Use a \"throwaway\" password since it's transmitted over plaintext for now $ balsam login # Follow the prompts to log into http://generic-01:8000 # With the credentials you used to register Create a Balsam site While the database is centrally located on the generic-01 host and shared, users will still create their own directories where Jobs run. Each Balsam execution directory is called a Site . A User can own many Sites across different HPC systems, or even have Sites configured on their laptop. Instead of creating a database with balsam init , we will create our first Balsam Site as follows: $ export BALSAM_LOG_LEVEL=WARNING # to avoid excessive logs on the CLI for now $ balsam site init my-site # Select the default configuration for Theta-KNL $ cd my-site $ balsam site ls # You should see the details of the Site that was just created Set up your Apps Every App in Balsam is now fully-specified by an ApplicationDefinition class that you write. You can add Apps to modules in the Site apps/ folder, with multiple apps per Python module file and multiple files. Every Site comes \"pre-packaged\" with some default Apps that Balsam developers have pre-configured for that particular HPC system. You are to modify, delete, and add your own apps. To make a new app, you can simply copy one of the existing App modules as a starting point, or you can use the command line interface to generate a new template app: $ balsam app create Application Name (of the form MODULE.CLASS): test.Hello Application Template [e.g. 'echo Hello {{ name }}!']: echo hello {{ name }} && sleep {{ sleeptime }} && echo goodbye Now open apps/test.py and see the Hello class that was generated for you. The allowed parameters for this App are given in double curly braces: {{ name }} and {{ sleeptime }} . When you add test.Hello jobs, you will have to pass these two parameters and Balsam will take care of building the command line. The other components of an App are also defined directly on the ApplicationDefinition class, rather than in other files: preprocess() will run on Jobs immediately before RUNNING postprocess() will run on Jobs immediately after RUN_DONE shell_preamble() takes the place of the envscript : return a multiline string envscript or a list of commands handle_timeout() will run immediately after RUN_TIMEOUT handle_error() will run immediately after RUN_ERROR Whenever you have changed your apps/ directory, you need to inform the API about the changes. All it takes is a single command to sync up: $ balsam app sync $ balsam app ls # Now you should see your newly-created App show up Note that the API does not store anything about the ApplicationDefinition classes other than the class name and some metadata about allowed parameters, allowed data transfers, etc... What actually runs at the Site is determined entirely from the class on the local filesystem. Add Jobs To create jobs from the CLI: # Get help: $ balsam job --help $ balsam job create --help # Create a couple jobs: $ balsam job create --app test.Hello --workdir test/1 --param name=\"world\" --param sleeptime=2 $ balsam job create --app test.Hello --workdir test/2 --param name=\"balsam\" --param sleeptime=1 To create jobs from the Python API: from balsam.api import Job , App , site_config hello_app = App . objects . get ( site_id = site_config . settings . site_id , class_path = \"test.Hello\" ) for i in range ( 10 ): job = Job ( f \"test-api/ { i } \" , hello_app . id , parameters = { \"name\" : \"testing!\" , \"sleeptime\" : \"3\" }, node_packing_count = 16 , ) job . save () Your jobs can be viewed from the CLI: $ balsam job ls Running the Site Any Balsam CLI or API interaction that you perform, other than the ones that bootstrap a new Site, does not affect the HPC system directly. Instead, a daemon running on your behalf at the Balsam Site pulls state changes from the API and applies that state within the local Site environment. sequenceDiagram User->>API: `balsam queue submit` API-->>User: OK Site->>API: Have any new BatchJobs for me? API-->>Site: Yes, here is one Site->>Cobalt: `qsub` this BatchJob Cobalt-->>Site: New Cobalt Job ID Site->>API: Update BatchJob with Cobalt Job ID API-->>Site: OK User->>API: `balsam queue ls` API-->>User: Updated BatchJob For instance, commands like balsam queue submit will not actually do anything with the queue if the Site is not running: they just update the central API. To start the Site: $ balsam site start The Site will run in the background, sync with API, perform qsub/qstat, initiate data transfers, and pre/post-process jobs. It can eventually be stopped with: $ balsam site stop Submitting a Job Now we can submit a BatchJob via the API and watch it appear in Cobalt: $ balsam queue submit -q debug-cache-quad -A datascience -n 1 -t 10 -j mpi $ watch \"qstat -u $USER && balsam queue ls\" When the BatchJob starts, an MPI mode launcher should run the jobs. You will see (copious) logs in the logs/ directory showing what's going on. Track the Job statuses with: $ balsam job ls All of the workdirs that are created will be visible under the data/ directory.","title":"ALCF-Theta Quickstart"},{"location":"tutorials/theta-quickstart/#running-on-theta","text":"","title":"Running on Theta"},{"location":"tutorials/theta-quickstart/#installation","text":"First create a new virtualenv and install it on Balsam: $ /soft/datascience/create_env.sh my-env # Or do it yourself $ source my-env/bin/activate $ git clone https://github.com/balsam-alcf/balsam.git $ cd balsam/ $ git checkout develop $ pip install -e .","title":"Installation"},{"location":"tutorials/theta-quickstart/#log-in","text":"Now you will need to create a Balsam user account and log in. Logging in fetches an access token that is used to identify you in subsequent API interactions, until the token expires and you have to log in again. $ balsam register Balsam server address: http://generic-01:8000 # Set your own username and password # Use a \"throwaway\" password since it's transmitted over plaintext for now $ balsam login # Follow the prompts to log into http://generic-01:8000 # With the credentials you used to register","title":"Log In"},{"location":"tutorials/theta-quickstart/#create-a-balsam-site","text":"While the database is centrally located on the generic-01 host and shared, users will still create their own directories where Jobs run. Each Balsam execution directory is called a Site . A User can own many Sites across different HPC systems, or even have Sites configured on their laptop. Instead of creating a database with balsam init , we will create our first Balsam Site as follows: $ export BALSAM_LOG_LEVEL=WARNING # to avoid excessive logs on the CLI for now $ balsam site init my-site # Select the default configuration for Theta-KNL $ cd my-site $ balsam site ls # You should see the details of the Site that was just created","title":"Create a Balsam site"},{"location":"tutorials/theta-quickstart/#set-up-your-apps","text":"Every App in Balsam is now fully-specified by an ApplicationDefinition class that you write. You can add Apps to modules in the Site apps/ folder, with multiple apps per Python module file and multiple files. Every Site comes \"pre-packaged\" with some default Apps that Balsam developers have pre-configured for that particular HPC system. You are to modify, delete, and add your own apps. To make a new app, you can simply copy one of the existing App modules as a starting point, or you can use the command line interface to generate a new template app: $ balsam app create Application Name (of the form MODULE.CLASS): test.Hello Application Template [e.g. 'echo Hello {{ name }}!']: echo hello {{ name }} && sleep {{ sleeptime }} && echo goodbye Now open apps/test.py and see the Hello class that was generated for you. The allowed parameters for this App are given in double curly braces: {{ name }} and {{ sleeptime }} . When you add test.Hello jobs, you will have to pass these two parameters and Balsam will take care of building the command line. The other components of an App are also defined directly on the ApplicationDefinition class, rather than in other files: preprocess() will run on Jobs immediately before RUNNING postprocess() will run on Jobs immediately after RUN_DONE shell_preamble() takes the place of the envscript : return a multiline string envscript or a list of commands handle_timeout() will run immediately after RUN_TIMEOUT handle_error() will run immediately after RUN_ERROR Whenever you have changed your apps/ directory, you need to inform the API about the changes. All it takes is a single command to sync up: $ balsam app sync $ balsam app ls # Now you should see your newly-created App show up Note that the API does not store anything about the ApplicationDefinition classes other than the class name and some metadata about allowed parameters, allowed data transfers, etc... What actually runs at the Site is determined entirely from the class on the local filesystem.","title":"Set up your Apps"},{"location":"tutorials/theta-quickstart/#add-jobs","text":"To create jobs from the CLI: # Get help: $ balsam job --help $ balsam job create --help # Create a couple jobs: $ balsam job create --app test.Hello --workdir test/1 --param name=\"world\" --param sleeptime=2 $ balsam job create --app test.Hello --workdir test/2 --param name=\"balsam\" --param sleeptime=1 To create jobs from the Python API: from balsam.api import Job , App , site_config hello_app = App . objects . get ( site_id = site_config . settings . site_id , class_path = \"test.Hello\" ) for i in range ( 10 ): job = Job ( f \"test-api/ { i } \" , hello_app . id , parameters = { \"name\" : \"testing!\" , \"sleeptime\" : \"3\" }, node_packing_count = 16 , ) job . save () Your jobs can be viewed from the CLI: $ balsam job ls","title":"Add Jobs"},{"location":"tutorials/theta-quickstart/#running-the-site","text":"Any Balsam CLI or API interaction that you perform, other than the ones that bootstrap a new Site, does not affect the HPC system directly. Instead, a daemon running on your behalf at the Balsam Site pulls state changes from the API and applies that state within the local Site environment. sequenceDiagram User->>API: `balsam queue submit` API-->>User: OK Site->>API: Have any new BatchJobs for me? API-->>Site: Yes, here is one Site->>Cobalt: `qsub` this BatchJob Cobalt-->>Site: New Cobalt Job ID Site->>API: Update BatchJob with Cobalt Job ID API-->>Site: OK User->>API: `balsam queue ls` API-->>User: Updated BatchJob For instance, commands like balsam queue submit will not actually do anything with the queue if the Site is not running: they just update the central API. To start the Site: $ balsam site start The Site will run in the background, sync with API, perform qsub/qstat, initiate data transfers, and pre/post-process jobs. It can eventually be stopped with: $ balsam site stop","title":"Running the Site"},{"location":"tutorials/theta-quickstart/#submitting-a-job","text":"Now we can submit a BatchJob via the API and watch it appear in Cobalt: $ balsam queue submit -q debug-cache-quad -A datascience -n 1 -t 10 -j mpi $ watch \"qstat -u $USER && balsam queue ls\" When the BatchJob starts, an MPI mode launcher should run the jobs. You will see (copious) logs in the logs/ directory showing what's going on. Track the Job statuses with: $ balsam job ls All of the workdirs that are created will be visible under the data/ directory.","title":"Submitting a Job"}]}